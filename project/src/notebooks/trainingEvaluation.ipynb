{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867f3872",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ““ TeleConnect â€” Training & Evaluation with MLflow\n",
    "\n",
    "This notebook documents **model training** and **evaluation** for the Telco Churn project.\n",
    "\n",
    "> **Start MLflow UI first:**\n",
    "> ```bash\n",
    "> mlflow ui --backend-store-uri ./mlruns --port 5000\n",
    "> ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc78f2",
   "metadata": {},
   "source": [
    "## 1) Environment & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "091a1c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook dir: /home/buntu/uni/MLOPS/project/src/notebooks\n",
      "Project root: /home/buntu/uni/MLOPS/project\n",
      "Modules path added: /home/buntu/uni/MLOPS/project/src/modules\n",
      "PROC_DIR: /home/buntu/uni/MLOPS/project/data/processed\n",
      "MLFLOW_URI: http://localhost:5000\n",
      "EXPERIMENT: telco-churn\n"
     ]
    }
   ],
   "source": [
    "# === Project root & import path config (ensures imports from src/modules work) ===\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for p in [cur, *cur.parents]:\n",
    "        if (p / \"mlruns\").exists():   # anchor folder present at project root\n",
    "            return p\n",
    "    return cur  # fallback: current dir\n",
    "\n",
    "NB_DIR = Path.cwd()\n",
    "PROJECT_ROOT = find_project_root(NB_DIR)\n",
    "print(\"Notebook dir:\", NB_DIR)\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "\n",
    "MODULES_DIR = PROJECT_ROOT / \"src\" / \"modules\"\n",
    "if str(MODULES_DIR) not in sys.path:\n",
    "    sys.path.append(str(MODULES_DIR))\n",
    "print(\"Modules path added:\", MODULES_DIR)\n",
    "\n",
    "# Common settings\n",
    "PROC_DIR = str(PROJECT_ROOT / \"data\" / \"processed\")\n",
    "MLFLOW_URI = \"http://localhost:5000\"\n",
    "EXPERIMENT_NAME = \"telco-churn\"\n",
    "\n",
    "print(\"PROC_DIR:\", PROC_DIR)\n",
    "print(\"MLFLOW_URI:\", MLFLOW_URI)\n",
    "print(\"EXPERIMENT:\", EXPERIMENT_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9300f8a",
   "metadata": {},
   "source": [
    "## 2) Quick sanity check of processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbe836ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/buntu/uni/MLOPS/project/data/processed/train.csv exists: True\n",
      "/home/buntu/uni/MLOPS/project/data/processed/val.csv exists: True\n",
      "/home/buntu/uni/MLOPS/project/data/processed/test.csv exists: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "for p in [os.path.join(PROC_DIR, \"train.csv\"),\n",
    "          os.path.join(PROC_DIR, \"val.csv\"),\n",
    "          os.path.join(PROC_DIR, \"test.csv\")]:\n",
    "    print(p, \"exists:\", os.path.exists(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b5d7636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using modules dir: /home/buntu/uni/MLOPS/project/src/modules\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "NB_DIR = Path.cwd()                 # this is src/notebooks\n",
    "MODULES_DIR = NB_DIR.parent / \"modules\"  # -> src/modules\n",
    "if str(MODULES_DIR) not in sys.path:\n",
    "    sys.path.append(str(MODULES_DIR))\n",
    "print(\"Using modules dir:\", MODULES_DIR)\n",
    "\n",
    "import modelBaselineMlflow as train_mod  # <- no 'src.' prefix here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880053ad",
   "metadata": {},
   "source": [
    "## 3) Train models with MLflow uses modelBaseline from modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "359ee1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/03 20:56:41 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/11/03 20:56:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run logreg at: http://localhost:5000/#/experiments/614247080687113769/runs/74d19fcfd5cf4844be139abdf6a0827d\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/614247080687113769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/03 20:57:01 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/11/03 20:57:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run random_forest at: http://localhost:5000/#/experiments/614247080687113769/runs/cc2911de6e1249c7be9cc218db4fbfd7\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/614247080687113769\n",
      "ðŸƒ View run xgboost_skipped at: http://localhost:5000/#/experiments/614247080687113769/runs/b9e16a3088f84bd7a9beeed57e8d045b\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/614247080687113769\n",
      "ðŸƒ View run lightgbm_skipped at: http://localhost:5000/#/experiments/614247080687113769/runs/808e749478d34229a7bb87a6248725e3\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/614247080687113769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/03 20:57:08 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/11/03 20:57:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run final_test_eval at: http://localhost:5000/#/experiments/614247080687113769/runs/b47645524cb3426b9ca837af7de2bda1\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/614247080687113769\n",
      "[OK] Entrenamiento y evaluaciÃ³n registrados en MLflow.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "argv_bak = sys.argv\n",
    "sys.argv = [\"modelBaselineMlflow.py\",\n",
    "            \"--proc_dir\", PROC_DIR,\n",
    "            \"--mlflow_uri\", MLFLOW_URI,\n",
    "            \"--experiment\", EXPERIMENT_NAME]\n",
    "try:\n",
    "    train_mod.main()\n",
    "finally:\n",
    "    sys.argv = argv_bak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63fa741",
   "metadata": {},
   "source": [
    "## 4) Load best run and print model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1d167b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run by val_f1: 74d19fcfd5cf4844be139abdf6a0827d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts:   0%|          | 0/1 [04:19<?, ?it/s]\n",
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 151.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Model loaded successfully (from 'model' stage)\n",
      "Estimator: LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Robust best-run selection + model loading using module function ===\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Import modelEvaluation module to use its load_model_from_run function\n",
    "MODULES_DIR = Path(PROJECT_ROOT) / \"src\" / \"modules\"\n",
    "if str(MODULES_DIR) not in sys.path:\n",
    "    sys.path.append(str(MODULES_DIR))\n",
    "\n",
    "import modelEvaluation as eval_mod\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "client = MlflowClient()\n",
    "\n",
    "# Get experiment and best run by val_f1\n",
    "exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "assert exp is not None, f\"Experiment '{EXPERIMENT_NAME}' not found at {MLFLOW_URI}.\"\n",
    "exp_id = exp.experiment_id\n",
    "\n",
    "runs = client.search_runs([exp_id], order_by=[\"metrics.val_f1 DESC\"], max_results=1)\n",
    "assert runs, \"No runs found. Train a model first.\"\n",
    "best = runs[0]\n",
    "best_run_id = best.info.run_id\n",
    "print(\"Best run by val_f1:\", best_run_id)\n",
    "\n",
    "# Use the module's load_model_from_run function (has robust fallback logic)\n",
    "# If best run is a TRAIN run it has 'model'; if it's 'final_test_eval', it has 'final_model'\n",
    "try:\n",
    "    model = eval_mod.load_model_from_run(best_run_id, model_stage=\"model\")\n",
    "    print(\"[OK] Model loaded successfully (from 'model' stage)\")\n",
    "except Exception as e:\n",
    "    # Try final_model if model fails\n",
    "    print(f\"[INFO] Trying 'final_model' stage: {str(e)[:100]}\")\n",
    "    model = eval_mod.load_model_from_run(best_run_id, model_stage=\"final_model\")\n",
    "    print(\"[OK] Model loaded successfully (from 'final_model' stage)\")\n",
    "\n",
    "# Print estimator class if it's a pipeline with 'clf' step\n",
    "try:\n",
    "    if hasattr(model, \"named_steps\") and \"clf\" in model.named_steps:\n",
    "        print(\"Estimator:\", model.named_steps[\"clf\"].__class__.__name__)\n",
    "    else:\n",
    "        print(\"Loaded object:\", type(model).__name__)\n",
    "except Exception as e:\n",
    "    print(\"Introspection error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c9c0d",
   "metadata": {},
   "source": [
    "## 5) Full evaluation (logs metrics & artifacts to MLflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: modelEvaluation.py [-h] [--proc_dir PROC_DIR] [--mlflow_uri MLFLOW_URI]\n",
      "                          [--experiment EXPERIMENT] [--run_id RUN_ID]\n",
      "                          [--model_stage {final_model,model}]\n",
      "                          [--n_splits N_SPLITS]\n",
      "                          [--avg_monthly_revenue AVG_MONTHLY_REVENUE]\n",
      "                          [--intervention_cost INTERVENTION_COST]\n",
      "                          [--intervention_rate INTERVENTION_RATE]\n",
      "                          [--retention_success_rate RETENTION_SUCCESS_RATE]\n",
      "modelEvaluation.py: error: unrecognized arguments: --skip_shap\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and log everything (metrics + plots) to MLflow\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# 1) Make sure we import from src/modules using the root we computed earlier\n",
    "MODULES_DIR = Path(PROJECT_ROOT) / \"src\" / \"modules\"\n",
    "if str(MODULES_DIR) not in sys.path:\n",
    "    sys.path.append(str(MODULES_DIR))\n",
    "\n",
    "# Reload the module to get the latest code with --skip_shap argument\n",
    "import modelEvaluation as eval_mod\n",
    "importlib.reload(eval_mod)  # This reloads the module with the latest code\n",
    "\n",
    "# 2) Call the module's main() by simulating CLI args\n",
    "argv_bak = sys.argv\n",
    "sys.argv = [\n",
    "    \"modelEvaluation.py\",\n",
    "    \"--proc_dir\", PROC_DIR,\n",
    "    \"--mlflow_uri\", MLFLOW_URI,\n",
    "    \"--experiment\", EXPERIMENT_NAME,\n",
    "\n",
    "    # ðŸ‘‡ ensure we evaluate the exact run we just selected\n",
    "    \"--run_id\", best_run_id,\n",
    "\n",
    "    # the best run you loaded is a training run -> artifact subdir is 'model'\n",
    "    \"--model_stage\", \"model\",\n",
    "\n",
    "    # business + CV knobs\n",
    "    \"--n_splits\", \"5\",\n",
    "    \"--avg_monthly_revenue\", \"850\",\n",
    "    \"--intervention_cost\", \"25\",\n",
    "    \"--intervention_rate\", \"0.30\",\n",
    "    \"--retention_success_rate\", \"0.25\",\n",
    "    \n",
    "    # ðŸ‘‡ Skip SHAP to avoid errors (temporary)\n",
    "    \"--skip_shap\",\n",
    "]\n",
    "try:\n",
    "    eval_mod.main()\n",
    "finally:\n",
    "    sys.argv = argv_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac53be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Fetch the latest \"evaluation_module\" run and display key metrics + plots in the notebook\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pathlib import Path\n",
    "import json, pandas as pd\n",
    "from IPython.display import display, Image\n",
    "\n",
    "client = MlflowClient()\n",
    "exp = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "assert exp is not None, f\"Experiment '{EXPERIMENT_NAME}' not found.\"\n",
    "\n",
    "# get most recent evaluation run\n",
    "runs = client.search_runs(\n",
    "    [exp.experiment_id],\n",
    "    filter_string=\"tags.mlflow.runName = 'evaluation_module'\",\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "assert runs, \"No evaluation_module run found. Run the evaluation cell first.\"\n",
    "eval_run = runs[0]\n",
    "eval_run_id = eval_run.info.run_id\n",
    "print(\"Evaluation run_id:\", eval_run_id)\n",
    "\n",
    "# 1) Collect metrics directly from MLflow\n",
    "metrics = eval_run.data.metrics  # a dict with all logged metrics\n",
    "# pick the most relevant to show\n",
    "pick = {k:v for k,v in metrics.items() if k.startswith(\"test_\") or k.startswith(\"temporal_cv_\") or k.startswith(\"bi_\")}\n",
    "df_metrics = pd.DataFrame([pick]).T\n",
    "df_metrics.columns = [\"value\"]\n",
    "display(df_metrics)\n",
    "\n",
    "# 2) Download + show artifacts (confusion matrix, ROC, PR, metrics.json, business_impact.json)\n",
    "dst = Path(\"src/notebooks/mlruns_artifacts\") / eval_run_id\n",
    "dst.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# helper to download and maybe display an image\n",
    "def get_artifact(path_rel, show_image=False):\n",
    "    try:\n",
    "        local_path = client.download_artifacts(eval_run_id, path_rel, dst_path=str(dst))\n",
    "        if show_image:\n",
    "            display(Image(filename=local_path))\n",
    "        return local_path\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {path_rel}: {e}\")\n",
    "\n",
    "# images\n",
    "get_artifact(\"test_confusion_matrix.png\", show_image=True)\n",
    "get_artifact(\"test_roc.png\", show_image=True)\n",
    "get_artifact(\"test_pr.png\", show_image=True)\n",
    "\n",
    "# JSONs\n",
    "test_metrics_json = get_artifact(\"test_metrics.json\", show_image=False)\n",
    "bi_json = get_artifact(\"business_impact.json\", show_image=False)\n",
    "\n",
    "# pretty print JSONs if present\n",
    "for p in [test_metrics_json, bi_json]:\n",
    "    if p:\n",
    "        with open(p) as f:\n",
    "            print(f\"\\n{Path(p).name}:\")\n",
    "            print(json.dumps(json.load(f), indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
